{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# My add-on : we need to install tqdm package which is missing in the FloydHub environment\n",
    "!pip install tqdm\n",
    "#!pip uninstall -y tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 14 21:31:59 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   49C    P8    30W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# My add-on : Check the GPU info (Some Memory GPU should be used)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 9999:\n",
      "Image - Min Value: 3 Max Value: 253\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7636564a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2#1\n",
    "sample_id = 9999 #5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Boolean Switch between 2 implementations\n",
    "    if(False):\n",
    "    \n",
    "        # TODO: Implement Function -> Min-Max Scaling \n",
    "        a = 0.1      # Set Min Scaled Value\n",
    "        b = 0.9      # Set Max Scaled Value\n",
    "        xmin = 0     # Image Pixel Min value \n",
    "        xmax = 255   # Image Pixel Max value \n",
    "        return ( a + (x - xmin) * ( b - a ) / ( xmax - xmin ) )\n",
    "\n",
    "    else :\n",
    "        # Implement Udacity's reviewer suggestion : \n",
    "        # normalize the image by dividing each pixels by the \n",
    "        # maximum value of the image pixels only\n",
    "\n",
    "        # Find the pixel max value per image \n",
    "        # this produce an array of the length of the image array\n",
    "        max_pixel = x.max(axis=(1,2,3), keepdims=True)\n",
    "\n",
    "        # we create a matrix of 1 of the same size as the array of images\n",
    "        # By multiplying it by 1/max_pixel and perform element wise multiplication\n",
    "        # between the image array and this matrix, each pixels in the array get\n",
    "        # divided by max_pixel value\n",
    "        norm_x =  x * np.ones(np.shape(x)) * (1/max_pixel) \n",
    "\n",
    "        return norm_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# Remark : \n",
    "# As x values are in 0 to 9, I don't see the point of maintaining a map of encodings outside the function.\n",
    "# Working with an array of 10 elements is straigtforward and provides the same encoding between each call  \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Boolean Switch between 2 implementations\n",
    "    if(False):\n",
    "        \n",
    "        # Initialiaze a Numpy array to store the one-hot encoded output (Shape is (lengh of x) * (9 possible values))\n",
    "        one_hot_labels = np.zeros((len(x), 10), dtype='uint8')\n",
    "\n",
    "        # Loop over each labels in the list...\n",
    "        for row_idx, label in enumerate(x):\n",
    "            # ... and perform the one_hot encoding for the current label\n",
    "            one_hot_labels[row_idx, label] = 1  \n",
    "\n",
    "        return  one_hot_labels\n",
    "\n",
    "    else:\n",
    "        # Implement Udacity's reviewer suggestion : \n",
    "        # You can also make use of sklearn's pre-processing functions to implement \n",
    "        # label's one-hot encodings which can be more efficient than loops. Check out \n",
    "        # Sklearn's LabelBinarizer and/or OneHotEncoder.\n",
    "        x=np.array(x).reshape(-1,1)\n",
    "        enc = OneHotEncoder(10, dtype='uint8', sparse=False)\n",
    "        enc.fit(x)\n",
    "        return(enc.transform(x).reshape(-1,10))\n",
    "\n",
    "                 \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function \n",
    "    return tf.placeholder(shape=(None, image_shape[0], image_shape[1], image_shape[2]), \n",
    "                          dtype=tf.float32, \n",
    "                          name=\"x\" )\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(shape=(None, n_classes), dtype=tf.int16, name=\"y\" )\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(shape=None, dtype=tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Prepare kernel size\n",
    "    h = conv_ksize[0]\n",
    "    w = conv_ksize[1]\n",
    "    depth_in = int(x_tensor.get_shape().as_list()[3]) # Images= 4D Tensor: Index Image, Height, Width, Input Depth\n",
    "    depth_out = conv_num_outputs\n",
    "\n",
    "    # Kernel's weights\n",
    "    kernel_weights = tf.Variable(tf.truncated_normal(shape=(h, w, depth_in, depth_out),\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     mean=0.0, stddev=0.05,\n",
    "                                                     seed=None), # Use seed to reproduce the results\n",
    "                                 name = \"conv_weights\")\n",
    "    \n",
    "    # Kernel's Bias \n",
    "    biases = tf.Variable(tf.truncated_normal(shape=[depth_out], \n",
    "                                             dtype=tf.float32, \n",
    "                                             mean=0.0, stddev=0.05,\n",
    "                                             seed=None),   # Use seed to reproduce the results\n",
    "                         name = \"conv_biases\")\n",
    "    \n",
    "    # Apply a convolution\n",
    "    kstride_h, kstride_w = conv_strides    \n",
    "    c = tf.nn.conv2d(x_tensor, kernel_weights, \n",
    "                     strides=[1, kstride_h, kstride_w, 1], \n",
    "                     padding='SAME',\n",
    "                     use_cudnn_on_gpu=True,\n",
    "                     name=\"conv_conv2d\")\n",
    "    \n",
    "    # Add Bias to the convolution\n",
    "    c = tf.nn.bias_add(c, biases, name=\"conv_add_bias\")\n",
    "    \n",
    "    # Add a nonlinear activation to the convolution\n",
    "    #c = tf.nn.relu(c, name=\"conv_relu\")\n",
    "    \n",
    "    # Apply Max Pooling using pool_ksize and pool_strides\n",
    "    pksize_h , pksize_w = pool_ksize\n",
    "    pstride_h, pstride_w = pool_strides\n",
    "    c = tf.nn.max_pool(c, \n",
    "                       ksize=[1, pksize_h, pksize_w, 1], \n",
    "                       strides=[1, pstride_h, pstride_w, 1], \n",
    "                       padding='SAME',\n",
    "                       name=\"conv_pool\")\n",
    "    \n",
    "    # Implement Udacity Reviewer's suggestion : \n",
    "    # Since relu(max_pool(x)) == max_pool(relu(x)) we can \n",
    "    # save 75% of the relu-operations by max-pooling first\n",
    "    c = tf.nn.relu(c, name=\"conv_relu\")\n",
    "\n",
    "    # Return the result of the convolution/maxpool in a Tensor \n",
    "    return c \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # get the image size from the last 3 dimensions of the x_tensor Tensor\n",
    "    # Note : we can not use the 1st dimension (batch size) to reshape \n",
    "    # the Tensor as the value could be 'None' (placeholder)\n",
    "    h = int(x_tensor.get_shape().as_list()[1])\n",
    "    w = int(x_tensor.get_shape().as_list()[2])\n",
    "    d = int(x_tensor.get_shape().as_list()[3])\n",
    "        \n",
    "    # Flatten the Tensor with a reshape \n",
    "    f = tf.reshape(x_tensor, shape=[ -1, h*w*d] , name=\"flatten\" )\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # get the flatten image size from the last 2 dimensions of the x_tensor Tensor\n",
    "    flat_img_size = int(x_tensor.get_shape().as_list()[1])\n",
    "    \n",
    "    # Prepare weigths variables\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=(flat_img_size, num_outputs),\n",
    "                                              dtype=tf.float32,\n",
    "                                              mean=0.0, stddev=0.05,\n",
    "                                              seed=None), # Use seed to reproduce the results\n",
    "                          name = \"full_weights\")\n",
    "    \n",
    "    # Prepare Bias variables\n",
    "    biases = tf.Variable(tf.truncated_normal(shape=[num_outputs],\n",
    "                                             dtype=tf.float32,\n",
    "                                             mean=0.0, stddev=0.05,\n",
    "                                             seed=None), # Use seed to reproduce the results\n",
    "                         name = \"full_biases\")\n",
    "    \n",
    "    # Apply feed forward pass\n",
    "    fc = tf.matmul(x_tensor, weights, name=\"full_conn\") + biases\n",
    "    \n",
    "    # Apply activation function\n",
    "    fc = tf.nn.relu(fc, name=\"full_relu\")\n",
    "    \n",
    "    # Return the result Tensor\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # get the flatten image size from the last 2 dimensions of the x_tensor Tensor\n",
    "    flat_img_size = int(x_tensor.get_shape().as_list()[1])\n",
    "    \n",
    "    # Prepare weigths variables\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=(flat_img_size, num_outputs),\n",
    "                                              dtype=tf.float32,\n",
    "                                              mean=0.0, stddev=0.05,\n",
    "                                              seed=None), # Use seed to reproduce the results\n",
    "                          name = \"output_weights\")\n",
    "    \n",
    "    # Prepare Bias variables\n",
    "    biases = tf.Variable(tf.truncated_normal(shape=[num_outputs],\n",
    "                                             dtype=tf.float32,\n",
    "                                             mean=0.0, stddev=0.05,\n",
    "                                             seed=None), # Use seed to reproduce the results\n",
    "                         name = \"output_biases\")\n",
    "    \n",
    "    # Apply feed forward pass\n",
    "    out = tf.matmul(x_tensor, weights, name=\"output\") + biases\n",
    "    \n",
    "    # Return the result Tensor\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Apply 1st convolution/pooling layer\n",
    "    conv1 = conv2d_maxpool(x , \n",
    "                           conv_num_outputs = 128, \n",
    "                           conv_ksize = (5, 5),\n",
    "                           conv_strides = (1,1),\n",
    "                           pool_ksize= (3,3),\n",
    "                           pool_strides= (2,2) )\n",
    "\n",
    "    # Apply 2nd convolution/pooling layer\n",
    "    conv2 = conv2d_maxpool(conv1 , \n",
    "                           conv_num_outputs = 480, #320\n",
    "                           conv_ksize = (3, 3),\n",
    "                           conv_strides = (1,1),   \n",
    "                           pool_ksize= (3,3),\n",
    "                           pool_strides= (2,2) )\n",
    " \n",
    "    # Apply 3rd convolution/pooling layer\n",
    "    conv3 = conv2d_maxpool(conv2 , \n",
    "                           conv_num_outputs = 640, \n",
    "                           conv_ksize = (2, 2),\n",
    "                           conv_strides = (1,1),\n",
    "                           pool_ksize= (3,3),\n",
    "                           pool_strides= (2,2) )\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Changing the dimensions of x_tensor from a 4-D tensor to a 2-D tensor \n",
    "    # to connect to the fully connected layer  \n",
    "    flat = flatten(conv3)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Apply first first fully connected layer\n",
    "    full1 = fully_conn(flat, num_outputs = 640)\n",
    "    \n",
    "    \n",
    "    # Apply Dropout\n",
    "    drop1 = tf.nn.dropout(full1, keep_prob)\n",
    "    \n",
    "    # Apply 2nd  fully connected layer\n",
    "    full2 = fully_conn(drop1, num_outputs = 128)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Apply output layer\n",
    "    out = output(full2, num_outputs = 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #Use the global variables \n",
    "    global valid_features \n",
    "    global valid_labels\n",
    "    \n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    valid_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    \n",
    " \n",
    "    print(\"Loss= \" , loss, \"    Validation Accuracy= \" , valid_accuracy)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 14 21:32:59 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   44C    P0    70W / 149W |  10873MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# My add-on : Check the GPU info (Some Memory GPU should be used)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss=  1.892     Validation Accuracy=  0.3438\n",
      "Epoch Running Time :  20.00801968574524\n",
      "\n",
      "Total Training Time  20.30398464202881\n"
     ]
    }
   ],
   "source": [
    "# Little add-on to enhance the monitoring\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        epoch_start= time.time()     # Little add-on to enhance the monitoring\n",
    "                \n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        # Little add-on to enhance the monitoring (Get to know the time to train one epoch)\n",
    "        elapsed_time =  time.time() - epoch_start \n",
    "        if (epoch==0): print(\"Epoch Running Time : \" , elapsed_time)\n",
    "\n",
    " # Little add-on to enhance the monitoring       \n",
    "train_time = time.time() - start_time\n",
    "print(\"\\nTotal Training Time \" ,  train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 90 #60\n",
    "batch_size = 256\n",
    "keep_probability = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss=  2.18045     Validation Accuracy=  0.26\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss=  1.6765     Validation Accuracy=  0.377\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss=  1.48754     Validation Accuracy=  0.4118\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss=  1.46203     Validation Accuracy=  0.479\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss=  1.34249     Validation Accuracy=  0.4856\n",
      "Epoch Running Time :  68.09335613250732\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss=  1.27662     Validation Accuracy=  0.5398\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss=  1.17024     Validation Accuracy=  0.544\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss=  0.893436     Validation Accuracy=  0.5668\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss=  0.977361     Validation Accuracy=  0.5888\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss=  0.856703     Validation Accuracy=  0.6014\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss=  0.822577     Validation Accuracy=  0.6288\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss=  0.817852     Validation Accuracy=  0.6128\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss=  0.588413     Validation Accuracy=  0.6536\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss=  0.700127     Validation Accuracy=  0.663\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss=  0.6361     Validation Accuracy=  0.6532\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss=  0.572482     Validation Accuracy=  0.6668\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss=  0.569855     Validation Accuracy=  0.6748\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss=  0.439598     Validation Accuracy=  0.6816\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss=  0.478407     Validation Accuracy=  0.6848\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss=  0.491857     Validation Accuracy=  0.658\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss=  0.33233     Validation Accuracy=  0.689\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss=  0.391475     Validation Accuracy=  0.6668\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss=  0.328644     Validation Accuracy=  0.7268\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss=  0.306251     Validation Accuracy=  0.7184\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss=  0.240641     Validation Accuracy=  0.7134\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss=  0.242158     Validation Accuracy=  0.7008\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss=  0.305603     Validation Accuracy=  0.6834\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss=  0.228464     Validation Accuracy=  0.7286\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss=  0.251324     Validation Accuracy=  0.7268\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss=  0.179972     Validation Accuracy=  0.7206\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss=  0.185352     Validation Accuracy=  0.7352\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss=  0.163361     Validation Accuracy=  0.7432\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss=  0.139167     Validation Accuracy=  0.7342\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss=  0.153919     Validation Accuracy=  0.7294\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss=  0.103798     Validation Accuracy=  0.757\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss=  0.129679     Validation Accuracy=  0.7432\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss=  0.107646     Validation Accuracy=  0.753\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss=  0.104042     Validation Accuracy=  0.758\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss=  0.0664302     Validation Accuracy=  0.7636\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss=  0.0855042     Validation Accuracy=  0.725\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss=  0.0943887     Validation Accuracy=  0.757\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss=  0.066971     Validation Accuracy=  0.7466\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss=  0.0644315     Validation Accuracy=  0.7698\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss=  0.0573501     Validation Accuracy=  0.7602\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss=  0.0421344     Validation Accuracy=  0.7666\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss=  0.0621555     Validation Accuracy=  0.74\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss=  0.0583988     Validation Accuracy=  0.7612\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss=  0.0421226     Validation Accuracy=  0.7612\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss=  0.0587789     Validation Accuracy=  0.7656\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss=  0.0414767     Validation Accuracy=  0.763\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss=  0.0569191     Validation Accuracy=  0.7734\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss=  0.0586658     Validation Accuracy=  0.7562\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss=  0.0222565     Validation Accuracy=  0.741\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss=  0.047128     Validation Accuracy=  0.7622\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss=  0.0191967     Validation Accuracy=  0.7638\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss=  0.0347587     Validation Accuracy=  0.7708\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss=  0.0202549     Validation Accuracy=  0.7734\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss=  0.0151369     Validation Accuracy=  0.7484\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss=  0.0213247     Validation Accuracy=  0.7696\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss=  0.0121486     Validation Accuracy=  0.7666\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss=  0.041478     Validation Accuracy=  0.7366\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss=  0.0123319     Validation Accuracy=  0.7656\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss=  0.0192496     Validation Accuracy=  0.752\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss=  0.0233222     Validation Accuracy=  0.7506\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss=  0.00692426     Validation Accuracy=  0.7822\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss=  0.0366264     Validation Accuracy=  0.7616\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss=  0.0102478     Validation Accuracy=  0.7712\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss=  0.0165521     Validation Accuracy=  0.7454\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss=  0.00943064     Validation Accuracy=  0.769\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss=  0.00540506     Validation Accuracy=  0.7644\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss=  0.0366433     Validation Accuracy=  0.7816\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss=  0.0137362     Validation Accuracy=  0.7442\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss=  0.0086119     Validation Accuracy=  0.7416\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss=  0.0172016     Validation Accuracy=  0.749\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss=  0.00396974     Validation Accuracy=  0.7684\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss=  0.0158893     Validation Accuracy=  0.7724\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss=  0.0104024     Validation Accuracy=  0.7748\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss=  0.00366275     Validation Accuracy=  0.7548\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss=  0.0112365     Validation Accuracy=  0.7726\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss=  0.00335657     Validation Accuracy=  0.7864\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss=  0.00720583     Validation Accuracy=  0.7566\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss=  0.0102156     Validation Accuracy=  0.7734\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss=  0.00480187     Validation Accuracy=  0.744\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss=  0.00648576     Validation Accuracy=  0.7704\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss=  0.00318105     Validation Accuracy=  0.7676\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss=  0.00602296     Validation Accuracy=  0.7646\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss=  0.00474739     Validation Accuracy=  0.7538\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss=  0.0022337     Validation Accuracy=  0.7742\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss=  0.0115614     Validation Accuracy=  0.7714\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss=  0.00444142     Validation Accuracy=  0.7804\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss=  0.00558969     Validation Accuracy=  0.7616\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss=  0.00194162     Validation Accuracy=  0.7756\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss=  0.00913234     Validation Accuracy=  0.7644\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss=  0.00563053     Validation Accuracy=  0.7662\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss=  0.000692802     Validation Accuracy=  0.7756\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss=  0.00679331     Validation Accuracy=  0.7692\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss=  0.00205603     Validation Accuracy=  0.771\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss=  0.000381012     Validation Accuracy=  0.7794\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss=  0.0109966     Validation Accuracy=  0.766\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss=  0.0507482     Validation Accuracy=  0.7752\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss=  0.00422072     Validation Accuracy=  0.7692\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss=  0.00267289     Validation Accuracy=  0.7836\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss=  0.00204374     Validation Accuracy=  0.776\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss=  0.0124212     Validation Accuracy=  0.7736\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss=  0.00307485     Validation Accuracy=  0.7576\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss=  0.00216844     Validation Accuracy=  0.7558\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss=  0.00199044     Validation Accuracy=  0.7678\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss=  0.000102447     Validation Accuracy=  0.7852\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss=  0.00176744     Validation Accuracy=  0.7846\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss=  0.0004629     Validation Accuracy=  0.7818\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss=  0.00585153     Validation Accuracy=  0.7752\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss=  0.00115514     Validation Accuracy=  0.7724\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss=  0.000322061     Validation Accuracy=  0.7804\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss=  0.00204279     Validation Accuracy=  0.7716\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss=  0.000637999     Validation Accuracy=  0.7758\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss=  0.00943487     Validation Accuracy=  0.756\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss=  0.000703859     Validation Accuracy=  0.7556\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss=  0.000433183     Validation Accuracy=  0.7808\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss=  0.00328073     Validation Accuracy=  0.778\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss=  0.000116786     Validation Accuracy=  0.7798\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss=  0.00904772     Validation Accuracy=  0.7758\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss=  0.00100943     Validation Accuracy=  0.7816\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss=  0.000486432     Validation Accuracy=  0.7822\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss=  0.00151345     Validation Accuracy=  0.7796\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss=  0.000237742     Validation Accuracy=  0.7752\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss=  0.00163771     Validation Accuracy=  0.7746\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss=  0.0018118     Validation Accuracy=  0.7832\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss=  3.38103e-05     Validation Accuracy=  0.781\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss=  0.00682739     Validation Accuracy=  0.7664\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss=  0.000463004     Validation Accuracy=  0.7788\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss=  0.000804602     Validation Accuracy=  0.7774\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss=  0.000537541     Validation Accuracy=  0.7794\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss=  0.000260397     Validation Accuracy=  0.7852\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss=  0.00138793     Validation Accuracy=  0.7706\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss=  0.000708387     Validation Accuracy=  0.7778\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss=  0.000848927     Validation Accuracy=  0.7876\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss=  0.00116167     Validation Accuracy=  0.7852\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss=  0.0005473     Validation Accuracy=  0.7782\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss=  0.00185039     Validation Accuracy=  0.769\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss=  0.00130174     Validation Accuracy=  0.7822\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss=  0.00109157     Validation Accuracy=  0.7948\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss=  0.00226484     Validation Accuracy=  0.7768\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss=  6.20179e-05     Validation Accuracy=  0.781\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss=  0.00303531     Validation Accuracy=  0.7628\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss=  0.000302768     Validation Accuracy=  0.7712\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss=  0.000464897     Validation Accuracy=  0.7762\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss=  0.00043381     Validation Accuracy=  0.7862\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss=  0.000263863     Validation Accuracy=  0.7776\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss=  0.00408555     Validation Accuracy=  0.775\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss=  0.0606775     Validation Accuracy=  0.708\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss=  0.000561568     Validation Accuracy=  0.769\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss=  0.000429548     Validation Accuracy=  0.787\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss=  9.84465e-05     Validation Accuracy=  0.7756\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss=  0.000247341     Validation Accuracy=  0.7682\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss=  0.000622221     Validation Accuracy=  0.7772\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss=  0.000957048     Validation Accuracy=  0.7804\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss=  0.000520545     Validation Accuracy=  0.7836\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss=  0.000110964     Validation Accuracy=  0.7908\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss=  0.000171265     Validation Accuracy=  0.7738\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss=  0.00033004     Validation Accuracy=  0.7792\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss=  0.000182097     Validation Accuracy=  0.7716\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss=  4.85402e-05     Validation Accuracy=  0.7902\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss=  0.000304465     Validation Accuracy=  0.785\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss=  0.000299217     Validation Accuracy=  0.7762\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss=  5.14483e-05     Validation Accuracy=  0.783\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss=  0.000269425     Validation Accuracy=  0.7794\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss=  0.00138405     Validation Accuracy=  0.7732\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss=  0.000151948     Validation Accuracy=  0.7782\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss=  0.000611789     Validation Accuracy=  0.7692\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss=  2.00193e-05     Validation Accuracy=  0.7864\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss=  0.00172245     Validation Accuracy=  0.7758\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss=  7.64432e-05     Validation Accuracy=  0.7828\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss=  2.75114e-05     Validation Accuracy=  0.781\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss=  0.000120207     Validation Accuracy=  0.7822\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss=  0.000726005     Validation Accuracy=  0.781\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss=  0.00038361     Validation Accuracy=  0.7732\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss=  0.000132742     Validation Accuracy=  0.7706\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss=  0.000152612     Validation Accuracy=  0.779\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss=  8.48234e-05     Validation Accuracy=  0.785\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss=  7.4646e-05     Validation Accuracy=  0.7822\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss=  0.000689402     Validation Accuracy=  0.777\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss=  0.000122724     Validation Accuracy=  0.7826\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss=  2.79161e-05     Validation Accuracy=  0.7804\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss=  8.15816e-05     Validation Accuracy=  0.7828\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss=  0.000127624     Validation Accuracy=  0.7856\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss=  0.000210436     Validation Accuracy=  0.78\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss=  0.000466159     Validation Accuracy=  0.7844\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss=  0.000219708     Validation Accuracy=  0.78\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss=  0.000122691     Validation Accuracy=  0.7876\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss=  5.77777e-05     Validation Accuracy=  0.7922\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss=  0.000107142     Validation Accuracy=  0.7882\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss=  0.00213091     Validation Accuracy=  0.773\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss=  8.8722e-05     Validation Accuracy=  0.7778\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss=  4.34647e-05     Validation Accuracy=  0.7782\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss=  5.1767e-05     Validation Accuracy=  0.7886\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss=  0.00336388     Validation Accuracy=  0.7882\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss=  0.000125484     Validation Accuracy=  0.7864\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss=  0.000141726     Validation Accuracy=  0.7758\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss=  4.24224e-05     Validation Accuracy=  0.7834\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss=  9.62035e-05     Validation Accuracy=  0.7772\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss=  0.000207898     Validation Accuracy=  0.7942\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss=  0.00110772     Validation Accuracy=  0.7676\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss=  0.000145261     Validation Accuracy=  0.786\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss=  8.37258e-05     Validation Accuracy=  0.7788\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss=  0.000254656     Validation Accuracy=  0.786\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss=  0.000136204     Validation Accuracy=  0.7846\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss=  0.000162921     Validation Accuracy=  0.786\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss=  0.00109279     Validation Accuracy=  0.7774\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss=  0.000319366     Validation Accuracy=  0.7846\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss=  0.000105831     Validation Accuracy=  0.7774\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss=  0.000249228     Validation Accuracy=  0.7848\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss=  0.00138107     Validation Accuracy=  0.7906\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss=  8.1737e-05     Validation Accuracy=  0.7826\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss=  6.41932e-05     Validation Accuracy=  0.7912\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss=  2.03221e-05     Validation Accuracy=  0.7864\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss=  0.000129792     Validation Accuracy=  0.7908\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss=  1.89244e-05     Validation Accuracy=  0.7952\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss=  2.76406e-05     Validation Accuracy=  0.7898\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss=  1.17502e-05     Validation Accuracy=  0.7904\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss=  4.38786e-05     Validation Accuracy=  0.78\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss=  5.36704e-05     Validation Accuracy=  0.7892\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss=  4.24355e-05     Validation Accuracy=  0.7924\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss=  0.000200712     Validation Accuracy=  0.7846\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss=  8.50826e-05     Validation Accuracy=  0.7784\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss=  1.6565e-05     Validation Accuracy=  0.79\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss=  7.43175e-05     Validation Accuracy=  0.7756\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss=  2.83898e-05     Validation Accuracy=  0.7842\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss=  0.000106695     Validation Accuracy=  0.7882\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss=  1.81391e-05     Validation Accuracy=  0.7944\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss=  0.000238083     Validation Accuracy=  0.7866\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss=  8.92907e-05     Validation Accuracy=  0.7836\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss=  1.094e-05     Validation Accuracy=  0.7852\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss=  5.68018e-06     Validation Accuracy=  0.7932\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss=  2.09424e-05     Validation Accuracy=  0.7776\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss=  0.000279388     Validation Accuracy=  0.7878\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss=  0.000501196     Validation Accuracy=  0.786\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss=  2.67615e-06     Validation Accuracy=  0.7976\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss=  0.000110342     Validation Accuracy=  0.7824\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss=  7.24781e-05     Validation Accuracy=  0.7864\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss=  0.000414324     Validation Accuracy=  0.7846\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss=  8.32084e-05     Validation Accuracy=  0.7734\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss=  0.000186644     Validation Accuracy=  0.7806\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss=  5.91091e-05     Validation Accuracy=  0.7846\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss=  0.000142764     Validation Accuracy=  0.7858\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss=  0.00250705     Validation Accuracy=  0.7838\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss=  0.000195571     Validation Accuracy=  0.7866\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss=  5.97636e-05     Validation Accuracy=  0.7828\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss=  0.000253934     Validation Accuracy=  0.785\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss=  0.000174674     Validation Accuracy=  0.7842\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss=  2.71436e-05     Validation Accuracy=  0.7838\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss=  3.80013e-05     Validation Accuracy=  0.7744\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss=  7.96952e-05     Validation Accuracy=  0.776\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss=  0.000399169     Validation Accuracy=  0.7768\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss=  0.000448572     Validation Accuracy=  0.7838\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss=  0.000381212     Validation Accuracy=  0.7794\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss=  0.000184564     Validation Accuracy=  0.7722\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss=  0.000259452     Validation Accuracy=  0.7844\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss=  0.00030551     Validation Accuracy=  0.774\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss=  2.06684e-05     Validation Accuracy=  0.7926\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss=  0.000548304     Validation Accuracy=  0.7806\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss=  0.000401577     Validation Accuracy=  0.7812\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss=  0.00104917     Validation Accuracy=  0.7772\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss=  5.11762e-05     Validation Accuracy=  0.7878\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss=  1.01025e-05     Validation Accuracy=  0.7768\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss=  2.51587e-05     Validation Accuracy=  0.7856\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss=  6.02813e-05     Validation Accuracy=  0.78\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss=  0.000105427     Validation Accuracy=  0.7746\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss=  0.000152688     Validation Accuracy=  0.7854\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss=  5.74771e-05     Validation Accuracy=  0.7868\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss=  2.01416e-05     Validation Accuracy=  0.7878\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss=  0.000609632     Validation Accuracy=  0.7824\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss=  0.000188595     Validation Accuracy=  0.7814\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss=  6.69231e-05     Validation Accuracy=  0.7916\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss=  4.69328e-05     Validation Accuracy=  0.7832\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss=  5.29258e-06     Validation Accuracy=  0.7848\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss=  3.50065e-05     Validation Accuracy=  0.7752\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss=  0.000739638     Validation Accuracy=  0.7816\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss=  0.00011209     Validation Accuracy=  0.7924\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss=  5.95728e-05     Validation Accuracy=  0.7756\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss=  2.29774e-06     Validation Accuracy=  0.7882\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss=  1.6477e-05     Validation Accuracy=  0.7678\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss=  4.34473e-05     Validation Accuracy=  0.7798\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss=  0.000214969     Validation Accuracy=  0.7828\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss=  3.97066e-05     Validation Accuracy=  0.7974\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss=  7.42631e-05     Validation Accuracy=  0.7966\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss=  0.000121166     Validation Accuracy=  0.7766\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss=  7.30792e-05     Validation Accuracy=  0.7854\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss=  3.12917e-06     Validation Accuracy=  0.7932\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss=  1.24715e-05     Validation Accuracy=  0.7898\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss=  7.32071e-05     Validation Accuracy=  0.7868\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss=  1.12249e-05     Validation Accuracy=  0.784\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss=  2.56296e-06     Validation Accuracy=  0.789\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss=  2.80436e-05     Validation Accuracy=  0.7904\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss=  0.000360905     Validation Accuracy=  0.7808\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss=  9.96168e-05     Validation Accuracy=  0.7804\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss=  1.3458e-05     Validation Accuracy=  0.7802\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss=  5.60164e-05     Validation Accuracy=  0.7782\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss=  1.35949e-05     Validation Accuracy=  0.7928\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss=  0.000405926     Validation Accuracy=  0.781\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss=  6.0389e-05     Validation Accuracy=  0.781\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss=  2.70789e-05     Validation Accuracy=  0.7758\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss=  2.04456e-05     Validation Accuracy=  0.7872\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss=  0.00103443     Validation Accuracy=  0.784\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss=  0.000276612     Validation Accuracy=  0.7872\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss=  8.51224e-05     Validation Accuracy=  0.7846\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss=  1.96948e-05     Validation Accuracy=  0.7866\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss=  2.63588e-05     Validation Accuracy=  0.7846\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss=  2.85623e-05     Validation Accuracy=  0.7954\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss=  0.000131659     Validation Accuracy=  0.7942\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss=  0.000223029     Validation Accuracy=  0.7936\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss=  3.87274e-05     Validation Accuracy=  0.7888\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss=  9.43488e-05     Validation Accuracy=  0.7798\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss=  6.52463e-05     Validation Accuracy=  0.7932\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss=  3.83979e-05     Validation Accuracy=  0.796\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss=  4.05334e-05     Validation Accuracy=  0.7874\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss=  0.000230058     Validation Accuracy=  0.778\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss=  0.000101774     Validation Accuracy=  0.8024\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss=  9.27035e-06     Validation Accuracy=  0.7896\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss=  8.12821e-05     Validation Accuracy=  0.7896\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss=  4.17156e-05     Validation Accuracy=  0.7914\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss=  0.000168966     Validation Accuracy=  0.791\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss=  0.0008444     Validation Accuracy=  0.7894\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss=  2.43144e-05     Validation Accuracy=  0.792\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss=  0.000160811     Validation Accuracy=  0.7886\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss=  2.0361e-05     Validation Accuracy=  0.7898\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss=  4.378e-05     Validation Accuracy=  0.7908\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss=  0.00109227     Validation Accuracy=  0.7872\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss=  0.000115474     Validation Accuracy=  0.7952\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss=  2.88012e-05     Validation Accuracy=  0.7912\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss=  0.000848168     Validation Accuracy=  0.7936\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss=  0.000161567     Validation Accuracy=  0.7908\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss=  0.000147669     Validation Accuracy=  0.795\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss=  7.57602e-05     Validation Accuracy=  0.7926\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss=  1.84128e-05     Validation Accuracy=  0.7944\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss=  5.89298e-05     Validation Accuracy=  0.7824\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss=  1.54976e-05     Validation Accuracy=  0.796\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss=  3.98808e-05     Validation Accuracy=  0.7842\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss=  2.26966e-05     Validation Accuracy=  0.7906\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss=  1.54611e-05     Validation Accuracy=  0.7946\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss=  3.73842e-05     Validation Accuracy=  0.794\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss=  3.27465e-05     Validation Accuracy=  0.7884\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss=  0.000551604     Validation Accuracy=  0.7886\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss=  0.00202492     Validation Accuracy=  0.7896\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss=  7.55776e-05     Validation Accuracy=  0.7896\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss=  4.66097e-06     Validation Accuracy=  0.7864\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss=  0.00119607     Validation Accuracy=  0.7814\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss=  0.000125916     Validation Accuracy=  0.7776\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss=  0.001221     Validation Accuracy=  0.7864\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss=  0.00161339     Validation Accuracy=  0.7864\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss=  1.40368e-06     Validation Accuracy=  0.783\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss=  0.000308137     Validation Accuracy=  0.793\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss=  0.000304084     Validation Accuracy=  0.791\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss=  0.000207026     Validation Accuracy=  0.7894\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss=  2.80521e-05     Validation Accuracy=  0.786\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss=  5.2214e-05     Validation Accuracy=  0.7834\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss=  5.12709e-05     Validation Accuracy=  0.79\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss=  2.69922e-05     Validation Accuracy=  0.7816\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss=  9.50451e-05     Validation Accuracy=  0.7798\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss=  0.000133126     Validation Accuracy=  0.7962\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss=  1.06413e-05     Validation Accuracy=  0.7828\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss=  0.000113964     Validation Accuracy=  0.7866\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss=  4.46429e-06     Validation Accuracy=  0.792\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss=  1.11545e-05     Validation Accuracy=  0.7906\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss=  0.00149931     Validation Accuracy=  0.7874\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss=  4.88638e-05     Validation Accuracy=  0.7882\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss=  9.19194e-05     Validation Accuracy=  0.794\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss=  3.09308e-05     Validation Accuracy=  0.7978\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss=  2.02653e-06     Validation Accuracy=  0.78\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss=  2.01529e-05     Validation Accuracy=  0.7952\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss=  2.63739e-05     Validation Accuracy=  0.7896\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss=  1.42357e-05     Validation Accuracy=  0.7912\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss=  6.08541e-06     Validation Accuracy=  0.7944\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss=  1.01764e-05     Validation Accuracy=  0.7892\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss=  6.86929e-06     Validation Accuracy=  0.7924\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss=  2.5926e-05     Validation Accuracy=  0.7838\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss=  0.000180547     Validation Accuracy=  0.7868\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss=  0.000286881     Validation Accuracy=  0.7858\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss=  5.29857e-05     Validation Accuracy=  0.7882\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss=  0.000298021     Validation Accuracy=  0.7996\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss=  2.56294e-06     Validation Accuracy=  0.7896\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss=  2.84769e-05     Validation Accuracy=  0.7882\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss=  8.0778e-05     Validation Accuracy=  0.7856\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss=  5.41169e-06     Validation Accuracy=  0.793\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss=  7.16201e-05     Validation Accuracy=  0.7944\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss=  6.41027e-06     Validation Accuracy=  0.7804\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss=  5.96081e-05     Validation Accuracy=  0.7852\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss=  2.34734e-05     Validation Accuracy=  0.7884\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss=  8.76373e-05     Validation Accuracy=  0.7894\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss=  0.000118952     Validation Accuracy=  0.7848\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss=  4.89941e-06     Validation Accuracy=  0.7932\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss=  0.000189222     Validation Accuracy=  0.789\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss=  2.42588e-06     Validation Accuracy=  0.7946\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss=  2.61957e-06     Validation Accuracy=  0.796\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss=  0.000907259     Validation Accuracy=  0.7846\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss=  1.77825e-05     Validation Accuracy=  0.7906\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss=  7.72826e-05     Validation Accuracy=  0.791\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss=  0.000220954     Validation Accuracy=  0.788\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss=  5.86192e-06     Validation Accuracy=  0.7856\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss=  4.55139e-05     Validation Accuracy=  0.7852\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss=  8.02295e-05     Validation Accuracy=  0.777\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss=  0.000548843     Validation Accuracy=  0.7896\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss=  0.000111735     Validation Accuracy=  0.7908\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss=  2.21963e-05     Validation Accuracy=  0.789\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss=  2.69812e-05     Validation Accuracy=  0.7876\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss=  1.5609e-05     Validation Accuracy=  0.7926\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss=  4.69242e-05     Validation Accuracy=  0.7898\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss=  1.1593e-06     Validation Accuracy=  0.7922\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss=  3.87464e-05     Validation Accuracy=  0.7832\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss=  0.00139509     Validation Accuracy=  0.7878\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss=  6.86361e-05     Validation Accuracy=  0.7846\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss=  7.11711e-05     Validation Accuracy=  0.7852\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss=  4.48812e-06     Validation Accuracy=  0.7814\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss=  0.000178368     Validation Accuracy=  0.786\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss=  1.96928e-05     Validation Accuracy=  0.7872\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss=  3.93973e-06     Validation Accuracy=  0.7782\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss=  0.000581026     Validation Accuracy=  0.781\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss=  0.000191072     Validation Accuracy=  0.7894\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss=  4.07163e-05     Validation Accuracy=  0.7892\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss=  0.000489414     Validation Accuracy=  0.7858\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss=  4.16921e-06     Validation Accuracy=  0.7828\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss=  6.56343e-05     Validation Accuracy=  0.7886\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss=  1.46135e-05     Validation Accuracy=  0.791\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss=  3.19644e-05     Validation Accuracy=  0.7878\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss=  6.09026e-05     Validation Accuracy=  0.793\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss=  1.65104e-06     Validation Accuracy=  0.7884\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss=  0.00271657     Validation Accuracy=  0.7886\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss=  3.52855e-05     Validation Accuracy=  0.797\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss=  8.85126e-07     Validation Accuracy=  0.7906\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss=  1.86149e-05     Validation Accuracy=  0.7876\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss=  1.57253e-05     Validation Accuracy=  0.7832\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss=  8.87721e-05     Validation Accuracy=  0.789\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss=  2.082e-05     Validation Accuracy=  0.7904\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss=  0.000223481     Validation Accuracy=  0.7764\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss=  0.000180805     Validation Accuracy=  0.7948\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss=  9.14094e-05     Validation Accuracy=  0.7854\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss=  8.48905e-05     Validation Accuracy=  0.7846\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss=  0.000184295     Validation Accuracy=  0.784\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss=  0.0182137     Validation Accuracy=  0.7848\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss=  0.000193877     Validation Accuracy=  0.7796\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss=  0.000686613     Validation Accuracy=  0.794\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss=  2.60818e-05     Validation Accuracy=  0.7874\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss=  4.71161e-05     Validation Accuracy=  0.7928\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss=  1.43654e-05     Validation Accuracy=  0.791\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss=  6.04621e-06     Validation Accuracy=  0.7842\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss=  5.18525e-06     Validation Accuracy=  0.7884\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss=  0.000244947     Validation Accuracy=  0.7912\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss=  0.000251718     Validation Accuracy=  0.785\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss=  9.47711e-07     Validation Accuracy=  0.7902\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss=  9.13986e-06     Validation Accuracy=  0.7846\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss=  1.67283e-05     Validation Accuracy=  0.7806\n",
      "\n",
      "Total Training Time  5749.917734622955\n"
     ]
    }
   ],
   "source": [
    "# Little add-on to enhance the monitoring\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        epoch_start= time.time()     # Little add-on to enhance the monitoring\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "        # Little add-on to enhance the monitoring (Get to know the time to train one epoch)\n",
    "        elapsed_time =  time.time() - epoch_start \n",
    "        if (epoch==0): print(\"Epoch Running Time : \" , elapsed_time)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    \n",
    "# Little add-on to enhance the monitoring       \n",
    "train_time = time.time() - start_time\n",
    "print(\"\\nTotal Training Time \" ,  train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.78408203125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncf8fqTxJHs/spY8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7b6457828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
